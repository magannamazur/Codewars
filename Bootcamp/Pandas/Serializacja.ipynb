{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c675fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co to jest serializacja i dlaczego powinno nas to obchodzić?\n",
    "# Pomyśl o przechowywaniu liczby całkowitej; jak chcesz to zapisać w pliku lub przesłać? To łatwe! \n",
    "# Możemy po prostu zapisać liczbę całkowitą do pliku i zapisać lub przesłać ten plik.\n",
    "\n",
    "# Ale co, jeśli pomyślimy o przechowywaniu obiektu Pythona (np. słownika Pythona lub Pandas DataFrame), \n",
    "# który ma złożoną strukturę i wiele atrybutów (np. kolumny i indeks DataFrame oraz typ danych każdej kolumny )? \n",
    "# W jaki sposób zapiszesz go jako plik lub prześlesz na inny komputer?\n",
    "\n",
    "# Tu właśnie pojawia się serializacja!\n",
    "\n",
    "# Serializacja to proces konwersji obiektu do formatu, który można przechowywać lub przesyłać.\n",
    "# Po przesłaniu lub zapisaniu serializowanych danych jesteśmy w stanie później zrekonstruować obiekt i\n",
    "# uzyskać dokładnie taką samą strukturę/obiekt, co sprawia, że ​​naprawdę wygodnie jest nam później korzystać z przechowywanego\n",
    "# obiektu zamiast rekonstruować obiekt od podstaw.\n",
    "\n",
    "# W Pythonie dostępnych jest wiele różnych formatów serializacji. Typowym przykładem map skrótów (słowników Pythona), które działają w wielu językach, jest format pliku JSON, który jest czytelny dla człowieka i pozwala nam przechowywać słownik i odtwarzać go z tą samą strukturą. Ale JSON może przechowywać tylko podstawowe struktury, takie jak lista i słownik, \n",
    "# i może przechowywać tylko ciągi znaków i liczby. Nie możemy poprosić JSON o zapamiętanie typu danych\n",
    "# (np. numpy float32 vs. float64). Nie można również odróżnić krotek Pythona od list.\n",
    "\n",
    "# Istnieją potężniejsze formaty serializacji. Poniżej przyjrzymy się dwóm popularnym bibliotekom serializacji w Pythonie, a mianowicie pickle i h5py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6085e991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\n",
      "ERROR: No matching distribution found for pickle\n"
     ]
    }
   ],
   "source": [
    "## sciagamy biblioteke pickle\n",
    "!pip install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf22c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c659d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tworzymy słownik i zapisujmy go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb0b4eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\"Hello\": \"World!\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482650ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4312961",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\"Hello\": \"World!\"}\n",
    "with open(\"test.pickle\", \"wb\") as outfile:\n",
    " \t# \"wb\" argument opens the file in binary mode\n",
    "\tpickle.dump(test_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12ae1260",
   "metadata": {},
   "outputs": [],
   "source": [
    "## wczytujemy go spowrotem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "915a5c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.pickle\", \"rb\") as infile:\n",
    "      test_dict_reconstructed = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e71d3d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hello': 'World!'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f8b9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict_ba = pickle.dumps(test_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9513dc7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x80\\x04\\x95\\x15\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94\\x8c\\x05Hello\\x94\\x8c\\x06World!\\x94s.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict_ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fc98966",
   "metadata": {},
   "outputs": [],
   "source": [
    "## zapisujemy modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff1e5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## można odpalić jeżeli zainstalujecie tensorflowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794bda8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d195761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1464 - accuracy: 0.9564 - val_loss: 0.0639 - val_accuracy: 0.9802\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0660 - accuracy: 0.9799 - val_loss: 0.0505 - val_accuracy: 0.9836\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0496 - accuracy: 0.9847 - val_loss: 0.0430 - val_accuracy: 0.9873\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0398 - accuracy: 0.9874 - val_loss: 0.0484 - val_accuracy: 0.9850\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0334 - accuracy: 0.9892 - val_loss: 0.0461 - val_accuracy: 0.9862\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0289 - accuracy: 0.9909 - val_loss: 0.0435 - val_accuracy: 0.9863\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0259 - accuracy: 0.9917 - val_loss: 0.0467 - val_accuracy: 0.9851\n",
      "[0.04301592707633972, 0.9872999787330627]\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\average_pooling2d\n",
      "......vars\n",
      "...layers\\average_pooling2d_1\n",
      "......vars\n",
      "...layers\\conv2d\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\flatten\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\mean_metric_wrapper\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-04 17:12:45         3652\n",
      "metadata.json                                  2023-04-04 17:12:45           64\n",
      "variables.h5                                   2023-04-04 17:12:46       776312\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-04 17:12:44         3652\n",
      "metadata.json                                  2023-04-04 17:12:44           64\n",
      "variables.h5                                   2023-04-04 17:12:46       776312\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\\average_pooling2d\n",
      "......vars\n",
      "...layers\\average_pooling2d_1\n",
      "......vars\n",
      "...layers\\conv2d\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\flatten\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\mean_metric_wrapper\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "[0.04301592707633972, 0.9872999787330627]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, AveragePooling2D, Dropout, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    " \n",
    "# Load MNIST digits\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    " \n",
    "# Reshape data to (n_samples, height, wiedth, n_channel)\n",
    "X_train = np.expand_dims(X_train, axis=3).astype(\"float32\")\n",
    "X_test = np.expand_dims(X_test, axis=3).astype(\"float32\")\n",
    " \n",
    "# One-hot encode the output\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    " \n",
    "# LeNet5 model\n",
    "model = Sequential([\n",
    "    Conv2D(6, (5,5), input_shape=(28,28,1), padding=\"same\", activation=\"tanh\"),\n",
    "    AveragePooling2D((2,2), strides=2),\n",
    "    Conv2D(16, (5,5), activation=\"tanh\"),\n",
    "    AveragePooling2D((2,2), strides=2),\n",
    "    Conv2D(120, (5,5), activation=\"tanh\"),\n",
    "    Flatten(),\n",
    "    Dense(84, activation=\"tanh\"),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])\n",
    " \n",
    "# Train the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "earlystopping = EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[earlystopping])\n",
    " \n",
    "# Evaluate the model\n",
    "print(model.evaluate(X_test, y_test, verbose=0))\n",
    " \n",
    "# Pickle to serialize and deserialize\n",
    "pickled_model = pickle.dumps(model)\n",
    "reconstructed = pickle.loads(pickled_model)\n",
    " \n",
    "# Evaluate again\n",
    "print(reconstructed.evaluate(X_test, y_test, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef44a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
