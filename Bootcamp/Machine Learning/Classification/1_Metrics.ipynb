{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-g4zNhPSX4BW"
   },
   "source": [
    "# Metryki klasyfikacji\n",
    "\n",
    "Aby poznać popularne metody oceny klasyfikatorów wygenerujemy sobie przykładowe sekwencje klas przykładów. Wykorzystamy w tym celu bibliotekę numpy oraz generator liczb losowych z rozkładu normalnego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = RandomState(30)\n",
    "\n",
    "random_1 = random.normal(loc = 0.0, size = 100)\n",
    "random_2 = random.logistic(loc = 1, size = 100)\n",
    "# loc = srodek rozkładu\n",
    "# size = wielkosc proby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.26405266  1.52790535 -0.97071094  0.47055962 -0.10069672  0.30379318\n",
      " -1.72596243  1.58509537  0.13429659 -1.10685547]\n",
      "[-0.34625296  2.7465497   0.46063247  3.2000744   0.12353606  1.56724961\n",
      "  2.32137633 -0.21323077  1.9572528   0.03456367]\n"
     ]
    }
   ],
   "source": [
    "print(random_1[:10])\n",
    "print(random_2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [1 if i >= 0 else 0 for i in random_1]\n",
    "y_pred = [1 if i >= 0 else 0 for i in random_2]\n",
    "# list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 0, 1, 0, 1, 1, 0]\n",
      "[0, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[:10])\n",
    "print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbK2vwiXX4Be"
   },
   "source": [
    "## Macierz błędów\n",
    "\n",
    "Pierwszym krokiem będzie stworzenie macierzy błędów, czyli wyznaczenia true postives, true negatives, false positives i false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(truth, prediction):\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for label_t, label_p in zip(truth,prediction):\n",
    "        if label_t == label_p:\n",
    "            if label_p ==1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else:\n",
    "            if label_p ==1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:30, TN:14\n",
      "Ile obserwacji zaklasyfikowaliśmy poprawnie: 44\n"
     ]
    }
   ],
   "source": [
    "print(f'TP:{tp}, TN:{tn}')\n",
    "print(f'Ile obserwacji zaklasyfikowaliśmy poprawnie: {tp+tn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP:43, FN:13\n",
      "Ile obserwacji zaklasyfikowaliśmy niepoprawnie: 56\n"
     ]
    }
   ],
   "source": [
    "print(f'FP:{fp}, FN:{fn}')\n",
    "print(f'Ile obserwacji zaklasyfikowaliśmy niepoprawnie: {fp+fn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUCbfBRoX4Bf"
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "Pierwszą miarą jest miara dokładności "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy = (tp+tn) / (tp+tn+fp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(truth, prediction):\n",
    "    tp, tn, fp, fn = confusion_matrix(truth, prediction)\n",
    "    return (tp+tn) / (tp+tn+fp+fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzN5wkknX4Bf"
   },
   "source": [
    "Wynik dla analizowanych predykcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność: 0.44\n"
     ]
    }
   ],
   "source": [
    "accuracy_1 = accuracy(y_test, y_pred)\n",
    "print(f'Dokładność: {accuracy_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDgmQCqzX4Bg"
   },
   "source": [
    "#### Problem\n",
    "Accuracy zachowuje się źle przy niezbalansowanych zbiorach. Wygenerujemy zbiór, w którym większość przykładów będzie negatywna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_3 = random.normal(loc = -1.5, size = 100)\n",
    "y_pred_2 = [1 if i >= 0 else 0 for i in random_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n"
     ]
    }
   ],
   "source": [
    "accuracy_2 = accuracy(y_test, y_pred_2)\n",
    "print(accuracy_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFio7HseX4Bh"
   },
   "source": [
    "Zakładając z góry, że wszystkie przykłady są negatywne uzyskujemy bardzo wysoką dokładność. Takie zachowanie jest w wielu przypadkach niepożądane - przypuśćmy, że próbujemy stworzyć klasyfikator do komórek rakowych, gdzie większość przypadków jest negatywna - dla takiego podejścia, zwrócenie informacji że wszystkie przypadki są negatywne da bardzo wysoką dokładność."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94\n"
     ]
    }
   ],
   "source": [
    "accuracy_3 = accuracy(100*[0], y_pred_2)\n",
    "print(accuracy_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Y6fcinkX4Bi"
   },
   "source": [
    "## Recall\n",
    "\n",
    "Ta miara z kolei mówi o tym, jaką część dodatnich wyników wykrył klasyfikator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall = tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(truth, prediction):\n",
    "    tp, tn, fp, fn = confusion_matrix(truth, prediction)\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6976744186046512"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_1 = recall(y_test, y_pred)\n",
    "recall_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046511627906976744"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_2 = recall(y_test, y_pred_2)\n",
    "recall_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall_2 wykrywa, że jest mały procent 1 (prawdziwych)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcIZ3w-LX4Bj"
   },
   "source": [
    "## Precision\n",
    "\n",
    "Jest to miara, która skupia się tylko na przykładach pozytywnych - mówi jaka część wyników wskazanych przez klasyfikator jako dodatnie jest rzeczywiście dodatnia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision = tp / (tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(truth, prediction):\n",
    "    tp, tn, fp, fn = confusion_matrix(truth, prediction)\n",
    "    return tp / (tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.410958904109589"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_1 = precision(y_test, y_pred)\n",
    "precision_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_2 = precision(y_test, y_pred_2)\n",
    "precision_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9SoQbRPX4Bk"
   },
   "source": [
    "# F1 Score\n",
    "\n",
    "Jest to średnia harmoniczna precyzji i czułości. Ogólnie - im wyższy F1-score tym lepszy jest klasyfikator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f_score = (2 * precision * recall)/(prec+rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(truth, prediction):\n",
    "    prec = precision(truth, prediction)\n",
    "    rec = recall(truth, prediction)\n",
    "    return (2 * prec * rec)/(prec+rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5172413793103448"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_1 = f1_score(y_test, y_pred)\n",
    "f1_score_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08163265306122448"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_2 = f1_score(y_test, y_pred_2)\n",
    "f1_score_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnZA6fKdX4Bm"
   },
   "source": [
    "**Zadanie:** Sprawdź funckje f1_score, precision_score, recall_score z modułu scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6976744186046512"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.410958904109589"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5172413793103448"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14, 43],\n",
       "       [13, 30]], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.25      0.33        57\n",
      "           1       0.41      0.70      0.52        43\n",
      "\n",
      "    accuracy                           0.44       100\n",
      "   macro avg       0.46      0.47      0.43       100\n",
      "weighted avg       0.47      0.44      0.41       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Metrics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
